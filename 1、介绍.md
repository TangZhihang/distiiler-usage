# distller 文档
## 什么是Distiller
distiller是一个用于神经网络压缩技术研究的开源python库
网络压缩可以减少神经网络的占地面积，提高推理速度，节约能量。Distiller提供了一个PyTorch环境，用于原型设计和分析压缩算法，如稀疏诱导方法和低精度算法。

Distiller模型压缩库包含了：系数算法（剪枝+正则化）+低精度算法（量化）
## 动机
稀疏张量是一些包含一些零的张量，但是稀疏张量通常只在包含相当数量的零时才有意义。稀疏神经网络使用一些(最好是许多)稀疏张量来执行计算。这些张量可以是参数(权重和偏差)或激活(特征映射)。<br>
为什么我们要关心稀疏性?<br>
今天的神经网络往往是深的，有数百万的权重和活动。参考GoogLeNet或ResNet50来获得一些例子。这些大型模型是计算密集型的，这意味着即使有专用的加速硬件，推理通过(网络评估)也需要时间。<br>
你可能认为延迟只是在某些情况下才会出现，比如自动驾驶系统，但事实上，每当我们人类与手机和电脑交互时，我们对交互的延迟很敏感。我们不喜欢等待搜索结果、应用程序或网页加载，我们对语音识别等实时交互特别敏感。推论延迟通常是我们想要最小化的东西。<br>

大型模型还需要大量内存，有数百万个参数。移动计算推理结果所需的所有数据会消耗能量，这在移动设备和服务器环境中都是一个问题。数据中心服务器机架受其功率包限，其ToC(总拥有成本)与其功耗和热特性相关。在移动设备环境中，我们显然一直都在关注设备电池的功耗。数据中心中的推理性能通常使用KPI(关键性能指标)来度量，其中包含了延迟和功率考虑因素:每秒推理、每瓦特推理(推理/秒/瓦特)。<br>

因为应用程序大小的限制和较长的应用程序下载时间,大型神经网络的存储和传输在移动设备环境中也是一个挑战。<br>

由于这些原因，我们希望尽可能地压缩网络，以减少所需的带宽和计算量。在神经网络模型中，通过正则化或剪枝来诱导稀疏是压缩网络的一种方法(量化是另一种方法)。稀疏神经网络具有速度快、体积小和能源效率高的优点。<br>
## 更小
利用张量元素由零控制的特点，可以对稀疏神经网络模型表示进行压缩。如果有的话，压缩格式，是非常HW和SW特定的，并且每个张量的最佳格式可能是不同的(一个明显的例子:很大密度的张量不应该被压缩)。计算硬件需要支持压缩格式，这样表示压缩才有意义。
压缩表示决策可能与使用tiles访问内存等算法交互。数据(例如参数张量)从主系统内存中读取/写入压缩，但是计算可以是密集的，也可以是稀疏的。在密集计算中，我们使用密集运算符，因此压缩的数据最终需要被解压成完整的、密集的大小。
我们能做的最好的就是把压缩的表示尽可能地接近计算引擎。<br>
另一方面，稀疏计算操作的是不需要解压的稀疏表示(因此我们区分稀疏表示和压缩表示)。
在HW中实现这不是一个简单的问题，通常意味着对向量化计算引擎的较低利用。因此，还有第三类表示，它们利用特定的硬件特性。例如，对于向量化的计算引擎，我们可以删除整个零权重向量并跳过它的计算(这使用了结构化剪枝或正则化)。

## 更快
现代神经网络中的许多层都是带宽限制的，这意味着执行延迟由可用带宽控制。本质上，硬件花费更多的时间将数据带到计算引擎，而不是实际执行计算。全连接层、RNNs和LSTMs是带宽控制操作的一些例子。
减少这些层所需的带宽，将立即加快它们的速度。<br>
一些剪枝算法从网络中删除整个内核、过滤器甚至层，而不会对最终的精度产生不利影响。根据硬件实现的不同，可以利用这些方法跳过计算，从而减少延迟和功耗。

## 更节能
因为与片内存储器(SRAM或高速缓存)相比，我们要多花费两个数量级的能量来访问片外存储器(例如DDR)，因此许多硬件设计采用了多层高速缓存层次结构。在这些芯片上的缓存中拟合网络的参数和激活可以在所需的带宽、总推断延迟和偏离轨道减少功耗方面产生很大的不同。
当然，如果我们使用稀疏或压缩表示，那么我们就减少了数据吞吐量，因此也减少了能源消耗。
##特点
1、该框架融合了剪枝,正则化及量化算法

2、一系列分析及评估压缩性能的工具

3、较流行压缩算法的应用