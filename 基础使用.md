下面是简单的例子使用蒸馏器的图像分类样本，显示一些蒸馏器的能力。

- 只训练
- 得到一个稀疏模型的参数统计量
- 量化后训练

示例：简单的仅训练的会话（没有压缩）
下面将在CIFAR10数据集上只调用名为“simplenet”的网络的训练(不进行压缩)。这大致是基于TorchVision的示例Imagenet训练应用程序的，因此如果您使用过该应用程序，它看起来应该很熟悉。在这个示例中，我们没有调用任何压缩机制:我们只是进行训练，因为对于修剪后的微调，训练是必不可少的部分。

请注意，第一次执行此命令时，CIFAR10代码将被下载到您的机器上，这可能需要一些时间——请让下载过程继续完成。
CIFAR10数据集的路径是任意的，但是在我们的示例中，我们将数据集放在与distiller相同的目录级别(即../../../data.cifar10)。
首先，切换到样例目录，然后调用应用程序:

```shell script
$ cd distiller/examples/classifier_compression
$ python3 compress_classifier.py --arch simplenet_cifar ../../../data.cifar10 -p 30 -j=1 --lr=0.01

```
您可以使用TensorBoard后端来查看训练进度(在下面的图表中，我们展示了两个具有不同LR值的训练会话)。对于压缩会话，我们添加了激活跟踪和参数稀疏级别，以及正则化丢失。
$ tensorboard --logdir=logs



示例:获得一个稀疏模型的参数统计量
我们在git存储库中包含了一些ResNet20模型的checkpoint，这些模型是我们用32位浮点数训练的。让我们载入一个模型的检查点，这个模型是我们用基于信道的套索正则化训练的。
通过以下命令行参数，示例应用程序加载模型(——resume)并打印关于模型权重的统计信息(——summary=稀疏性)。如果您想要加载以前修剪过的模型，例如检查权重稀疏性统计信息，这是很有用的。
注意，当你恢复一个存储的检查点时，你仍然需要告诉应用程序检查点使用的是哪种网络架构(-a=resnet20_cifar):

```shell script
$ python3 compress_classifier.py --resume=../ssl/checkpoints/checkpoint_trained_ch_regularized_dense.pth.tar -a=resnet20_cifar ../../../data.cifar10 --summary=sparsity
```
您应该会看到一个文本表，详细说明参数张量的各种稀疏度。第一列是参数名称，后面是它的形状、稠密模型和稀疏模型中非零元素的数量(NNZ)。下一组列显示列级、行级、通道级、内核级、过滤器级和元素级的稀疏。
最后是元素绝对值的标准偏差、平均值和平均值。
在压缩洞察笔记本中，我们使用matplotlib来绘制这个摘要的条形图，这确实显示了没有令人印象深刻的足迹压缩。
虽然压缩的内存占用很低，但这个模型实际上节省了26.6%的MACs计算。


例子:训练后量化
这个例子为CIFAR10执行ResNet20的8位量化。我们在git存储库中包含了一个ResNet20模型的检查点，这个模型是我们用32位浮点数训练的，所以我们将使用这个模型并量化它:
```shell script
$ python3 compress_classifier.py -a resnet20_cifar ../../../data.cifar10 --resume ../ssl/checkpoints/checkpoint_trained_dense.pth.tar --quantize-eval --evaluate
```
上面的命令行将保存一个名为quantized_checkpoint.pth.tar的检查点，其中包含量化的模型参数。[这里](https://github.com/NervanaSystems/distiller/blob/master/examples/quantization/post_train_quant/command_line.md) 有更多的例子。
