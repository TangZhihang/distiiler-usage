#准备量化模型
##背景
注:如果你只是想要一个需要的修改，以确保模型在Distiller中适当量化，你可以跳过这部分和直接进入下一部分。

Distiller提供了一种自动机制，可将“普通”FP32 PyTorch模型转换为量化模型(用于[量化感知培训](https://nervanasystems.github.io/distiller/schedule.html#quantization-aware-training)和[训练后量化](https://nervanasystems.github.io/distiller/schedule.html#post-training-quantization))。这个机制在PyTorch“模块”级别工作。“模块”指的是torch.nn的任何子类。模块类。Distiller量化器可以检测模块，并将其替换为其他模块。

但是，在PyTorch中并不要求将所有操作定义为模块。操作通常通过直接重载张量操作符(+、-等)和torch命名空间下的函数(例如torch.cat())执行。还有torch.nn.functional命令空间，它提供与torch.nn中提供的模块在功能上等价的功能。
当一个操作不维护任何状态时，即使它有一个专用的nn.Module，它仍经常通过其功能对等物调用。例如，调用nn.function.relu()而不是创建nn.ReLU的实例来调用它。
这些非模块操作直接从模块的forward函数调用。有一些方法可以预先发现这些操作，这些操作在蒸馏器中[用于不同的目的](https://github.com/NervanaSystems/distiller/blob/master/distiller/summary_graph.py)。即使这样，我们也不能不借助那些“肮脏的” Python技巧来替换这些操作，出于多种原因，我们宁愿不这样做。


此外，在forward函数中可能会多次重用相同的模块实例。这对Distiller来说也是个问题。如果对某个操作的每个调用没有“绑定”到专用模块实例，那么有几个流将不能按预期工作。例如:

+ 在收集统计信息时，重复使用的每次调用都会覆盖为前一次调用收集的统计信息。最后，除了最后一个调用，所有调用的统计信息都丢失了。
+ [“网络感知”量化](https://github.com/NervanaSystems/distiller/blob/master/examples/quantization/post_train_quant/command_line.md#net-aware-quantization)依赖于从模型中执行的每个操作到调用它的模块的1:1映射。有了可重用的模块，这种映射就不再是1:1了。

因此，为了确保一个模型中所有支持的操作都被Distiller适当地量化，可能有必要在将模型代码传递给量化器之前对其进行修改。注意，受支持的操作的确切集合在不同的[可用量化器](https://nervanasystems.github.io/distiller/algo_quantization.htm)之间可能有所不同。

##模型准备待办事项表
1. 准备量化模型所需的步骤可总结如下:
用模代替直接张量运算
2. 用专用实例替换重用的模块
3. 调用具有等效模块的函数取代torch.nn。
4. 特殊情况——用可量化变量替换不能量化的模块

在下一节中，我们将看到这个列表中1-3项的示例。

至于“特殊情况”，目前唯一的这种情况是LSTM。有关详细信息，请参阅示例后面的部分。

##模型准备的例子
我们将使用以下简单模块作为示例。这个模块松散地基于torchvision中的ResNet实现，有一些没有多大意义的更改，旨在演示可能需要的不同修改。
```
import torch.nn as nn
import torch.nn.functional as F

class BasicModule(nn.Module):
    def __init__(self, in_ch, out_ch, kernel_size):
        super(BasicModule, self).__init__()
        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size)
        self.bn1 = nn.BatchNorm2d(out_ch)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size)
        self.bn2 = nn.BatchNorm2d(out_ch)

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)

        # (1) Overloaded tensor addition operation
        # Alternatively, could be called via a tensor function: skip_1.add_(identity)
        out += identity
        # (2) Relu module re-used
        out = self.relu(out)

        # (3) Using operation from 'torch' namespace
        out = torch.cat([identity, out], dim=1)
        # (4) Using function from torch.nn.functional
        out = F.sigmoid(out)

        return out
```
###用modules代替直接张量运算
前向函数中的加法(1)和连接(3)运算就是直接张量运算的例子。这些运算没有在torch.nn.module中定义的等价模块。
因此，如果我们想量化这些操作，我们必须实现调用它们的模块。在Distiller中，我们为常见操作实现了几个简单的包装模块。这些是在distiller.modules名称空间中定义的。具体来说，应该用EltWiseAdd模块替换加法操作，用Concat模块替换串联操作。点击[此处](https://github.com/NervanaSystems/distiller/tree/master/distiller/modules)的代码，查看可用的模块。

###用专用实例替换重用的模块
上面的relu操作是通过模块调用的，但是两个调用使用的是相同的实例(2)。我们需要在__init__中创建第二个nn.ReLU的实例，并在前向传播中的第二次调用时使用它。

###调用等效的模块取代```torch.nn.functional ```
使用函数接口调用sigmoid(4)操作。幸运的是，torch.nn.functional操作有等价模块，所以我们可以直接使用它。在这种情况下，我们需要创建一个torch.nn.sigmoid的实例。


###将所有放在一起

在完成以上所有细节的更改后，我们得到:
```
import torch.nn as nn
import torch.nn.functional as F

class BasicModule(nn.Module):
    def __init__(self, in_ch, out_ch, kernel_size):
        super(BasicModule, self).__init__()
        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size)
        self.bn1 = nn.BatchNorm2d(out_ch)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size)
        self.bn2 = nn.BatchNorm2d(out_ch)

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)

        # (1) Overloaded tensor addition operation
        # Alternatively, could be called via a tensor function: skip_1.add_(identity)
        out += identity
        # (2) Relu module re-used
        out = self.relu(out)

        # (3) Using operation from 'torch' namespace
        out = torch.cat([identity, out], dim=1)
        # (4) Using function from torch.nn.functional
        out = F.sigmoid(out)

        return out
Replace direct tensor operations with modules
The addition (1) and concatenation (3) operations in the forward function are examples of direct tensor operations. These operations do not have equivalent modules defined in torch.nn.Module. Hence, if we want to quantize these operations, we must implement modules that will call them. In Distiller we've implemented a few simple wrapper modules for common operations. These are defined in the distiller.modules namespace. Specifically, the addition operation should be replaced with the EltWiseAdd module, and the concatenation operation with the Concat module. Check out the code here to see the available modules.

Replace re-used modules with dedicated instances
The relu operation above is called via a module, but the same instance is used for both calls (2). We need to create a second instance of nn.ReLU in __init__ and use that for the second call during forward.

Replace torch.nn.functional calls with equivalent modules
The sigmoid (4) operation is invoked using the functional interface. Luckily, operations in torch.nn.functional have equivalent modules, so se can just use those. In this case we need to create an instance of torch.nn.Sigmoid.

Putting it all together
After making all of the changes detailed above, we end up with:

import torch.nn as nn
import torch.nn.functional as F
import distiller.modules

class BasicModule(nn.Module):
    def __init__(self, in_ch, out_ch, kernel_size):
        super(BasicModule, self).__init__()
        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size)
        self.bn1 = nn.BatchNorm2d(out_ch)
        self.relu1 = nn.ReLU()
        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size)
        self.bn2 = nn.BatchNorm2d(out_ch)

        # Fixes start here
        # (1) Replace '+=' with an inplace module
        self.add = distiller.modules.EltWiseAdd(inplace=True)
        # (2) Separate instance for each relu call
        self.relu2 = nn.ReLU()
        # (3) Dedicated module instead of tensor op
        self.concat = distiller.modules.Concat(dim=1)
        # (4) Dedicated module instead of functional call
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu1(out)
        out = self.conv2(out)
        out = self.bn2(out)

        out = self.add(out, identity)
        out = self.relu(out)
        out = self.concat(identity, out)
        out = self.sigmoid(out)

        return out
```

##特殊情况:LSTM(一个“复合”模块)
### 背景
LSTMs提供了一种特殊情况。在torch.nn中，LSTM块由全连通层和s型/tanh非线性等基本构件组成，它们都具有专用模块。但是，PyTorch中提供的LSTM实现并不使用这些构建块。
为了优化目的，所有的内部操作都在c++级别实现。在Python级别公开的模型的唯一部分是完全连接的层的参数。因此，PyTorch LSTM模块所能做的就是量化整个块的输入/输出，以及量化FC层参数。
我们根本无法将物体的内部阶段量化。除了只对内部阶段进行量化之外，我们还希望能够分别控制每个内部阶段的量化参数。

###该做什么
Distiller提供了LSTM的“模块化”实现，完全由Python级别定义的操作组成。我们提供了一个实现DistillerLSTM和DistillerLSTMCell，并行PyTorch提供的LSTM和LSTMCell。请参见[此处](https://github.com/NervanaSystems/distiller/blob/master/distiller/modules/rnn.py)的实现。

还提供了一个将模型中的所有LSTM实例转换为Ditiller变体的函数:
```
model = distiller.modules.convert_model_to_distiller_lstm(model)
```

要查看这个转换和LSTM块中混合精度量化的示例，请在[这里](https://github.com/NervanaSystems/distiller/blob/master/examples/word_language_model/quantize_lstm.ipynb)查看我们关于单词语言模型量化的教程。