#Compression scheduler

在迭代修剪中，我们创建了一种修剪方案，该修剪方案指定如何修剪以及在修剪和培训阶段的每个阶段修剪什么。 这激励了CompressionScheduler的设计：它需要成为训练循环的一部分，并且能够制定和实施修剪，正则化和量化决策。 我们希望能够更改压缩时间表的细节，而无需触摸代码，并决定使用YAML作为该规范的容器。 我们发现，当我们在同一代码库上进行许多实验时，如果将差异与代码库分离，则维护所有这些实验会变得更加容易。 因此，我们增加了对学习速率衰减调度的调度程序支持，并且，我们希望自由地更改LR衰减策略而不更改代码。

##High level overview
让我们简要地讨论主要机制和抽象：调度规范由定义Pruner，Regularizer，Quantizer，LR-Scheduler和Policys实例的部分列表组成。


* 修剪器，正则化器和量化器非常相似：它们分别实现修剪/正则化/量化算法。
* LR调度器指定LR衰减算法。

这些定义了时间表的哪一部分。

该策略定义了计划的何时部分：在哪个时期开始应用Pruner / Regularizer / Quantizer / LR-decay，该时期结束，以及调用策略的频率（应用频率）。 策略还定义了它正在管理的Pruner / Regularizer / Quantizer / LR-decay的实例。
从YAML文件或从字典配置CompressionScheduler，但是您也可以从代码手动创建Policy，Pruner，Regularizer和Quantizer。

##Syntax through example
我们将使用alexnet.schedule_agp.yaml来解释一些用于配置Alexnet敏感度修剪的YAML语法。
```yaml
version: 1
pruners:
  my_pruner:
    class: 'SensitivityPruner'
    sensitivities:
      'features.module.0.weight': 0.25
      'features.module.3.weight': 0.35
      'features.module.6.weight': 0.40
      'features.module.8.weight': 0.45
      'features.module.10.weight': 0.55
      'classifier.1.weight': 0.875
      'classifier.4.weight': 0.875
      'classifier.6.weight': 0.625

lr_schedulers:
   pruning_lr:
     class: ExponentialLR
     gamma: 0.9

policies:
  - pruner:
      instance_name : 'my_pruner'
    starting_epoch: 0
    ending_epoch: 38
    frequency: 2

  - lr_scheduler:
      instance_name: pruning_lr
    starting_epoch: 24
    ending_epoch: 200
    frequency: 1
```

YAML语法只有一个版本，并且该版本号目前尚未验证。 但是，为了适应未来的发展，最好是让YAML解析器知道您正在使用版本1语法，以防出现版本2。

```yaml
version: 1
```
在pruners部分，我们定义了希望调度程序实例化和使用的pruners实例。
我们定义了一个名为my_pruner的pruner实例。我们将在策略部分引用这个实例。
然后我们列出每个权值张量的灵敏度乘子s。
您可以在本节中列出任意数量的修剪器，只要每个修剪器有一个惟一的名称即可。您可以在一个调度中使用多种类型的修剪器。
```yaml
pruners:
  my_pruner:
    class: 'SensitivityPruner'
    sensitivities:
      'features.module.0.weight': 0.25
      'features.module.3.weight': 0.35
      'features.module.6.weight': 0.40
      'features.module.8.weight': 0.45
      'features.module.10.weight': 0.55
      'classifier.1.weight': 0.875
      'classifier.4.weight': 0.875
      'classifier.6.weight': 0.6
```

接下来，我们希望在lr_schedulers部分中指定学习速率衰减调度。我们为这个实例分配一个名称:pruning_lr。与pruners部分一样，您可以使用任何名称，
只要所有lr -调度器都有一个惟一的名称。目前，只允许LR-scheduler的一个实例。LR-scheduler必须是PyTorch的[_LRScheduler](https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html) 的子类。
您可以使用torch.optim中定义的任何调度器。lr_scheduler([见这里](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) )。此外，我们还在Distiller中实现了一些额外的调度器([参见这里](https://github.com/NervanaSystems/distiller/blob/master/distiller/learning_rate.py) )。
关键字参数直接传递给LR-scheduler的构造函数，以便在添加新的LR-scheduler时添加到torch.optim。lr_scheduler，
可以在不更改应用程序代码的情况下使用它们。

```yaml
lr_schedulers:
   pruning_lr:
     class: ExponentialLR
     gamma: 0.9
```
最后，我们定义了policy部分，它定义了实际的调度。策略通过命名实例来管理一个Pruner、Regularizer、Quantizer或LRScheduler的实例。在下例中，PruningPolicy使用名为my_pruner的pruner实例:它以2个epoch(即每隔一个epoch)的频率激活它，从epoch 0开始，到epoch 38结束。
```yaml
policies:
  - pruner:
      instance_name : 'my_pruner'
    starting_epoch: 0
    ending_epoch: 38
    frequency: 2

  - lr_scheduler:
      instance_name: pruning_lr
    starting_epoch: 24
    ending_epoch: 200
    frequency: 1
```
这是迭代裁剪:
1. 训练连接
2. 裁剪连接
3. 再次训练权值
4. 转到2

[ Learning both Weights and Connections for Efficient Neural Networks](https://arxiv.org/abs/1506.02626) 这篇论文描述了它。
"Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections...After an initial training phase, we remove all connections whose weight is lower than a threshold. This pruning converts a dense, fully-connected layer to a sparse layer. This first phase learns the topology of the networks — learning which connections are important and removing the unimportant connections. We then retrain the sparse network so the remaining connections can compensate for the connections that have been removed. The phases of pruning and retraining may be repeated iteratively to further reduce network complexity."


##Regularization

您还可以定义和安排正则化。
###L1正则化
格式(这是一个非正式的规范，不是一个有效的[ABNF](https://en.wikipedia.org/wiki/Augmented_Backus%E2%80%93Naur_form)规范):
```shell script
regularizers:
  <REGULARIZER_NAME_STR>:
    class: L1Regularizer
    reg_regims:
      <PYTORCH_PARAM_NAME_STR>: <STRENGTH_FLOAT>
      ...
      <PYTORCH_PARAM_NAME_STR>: <STRENGTH_FLOAT>
    threshold_criteria: [Mean_Abs | Max]
```
例如：
```yaml
version: 1

regularizers:
  my_L1_reg:
    class: L1Regularizer
    reg_regims:
      'module.layer3.1.conv1.weight': 0.000002
      'module.layer3.1.conv2.weight': 0.000002
      'module.layer3.1.conv3.weight': 0.000002
      'module.layer3.2.conv1.weight': 0.000002
    threshold_criteria: Mean_Abs

policies:
  - regularizer:
      instance_name: my_L1_reg
    starting_epoch: 0
    ending_epoch: 60
    frequency: 1
```
###组正则化
格式(非正式规范):
```
Format:
  regularizers:
    <REGULARIZER_NAME_STR>:
      class: L1Regularizer
      reg_regims:
        <PYTORCH_PARAM_NAME_STR>: [<STRENGTH_FLOAT>, <'2D' | '3D' | '4D' | 'Channels' | 'Cols' | 'Rows'>]
        <PYTORCH_PARAM_NAME_STR>: [<STRENGTH_FLOAT>, <'2D' | '3D' | '4D' | 'Channels' | 'Cols' | 'Rows'>]
      threshold_criteria: [Mean_Abs | Max]
```
例如：
```yaml
version: 1

regularizers:
  my_filter_regularizer:
    class: GroupLassoRegularizer
    reg_regims:
      'module.layer3.1.conv1.weight': [0.00005, '3D']
      'module.layer3.1.conv2.weight': [0.00005, '3D']
      'module.layer3.1.conv3.weight': [0.00005, '3D']
      'module.layer3.2.conv1.weight': [0.00005, '3D']
    threshold_criteria: Mean_Abs

policies:
  - regularizer:
      instance_name: my_filter_regularizer
    starting_epoch: 0
    ending_epoch: 60
    frequency: 1
```
##混合起来
你可以混合修剪和规则化。
```yaml
version: 1
pruners:
  my_pruner:
    class: 'SensitivityPruner'
    sensitivities:
      'features.module.0.weight': 0.25
      'features.module.3.weight': 0.35
      'features.module.6.weight': 0.40
      'features.module.8.weight': 0.45
      'features.module.10.weight': 0.55
      'classifier.1.weight': 0.875
      'classifier.4.weight': 0.875
      'classifier.6.weight': 0.625

regularizers:
  2d_groups_regularizer:
    class: GroupLassoRegularizer
    reg_regims:
      'features.module.0.weight': [0.000012, '2D']
      'features.module.3.weight': [0.000012, '2D']
      'features.module.6.weight': [0.000012, '2D']
      'features.module.8.weight': [0.000012, '2D']
      'features.module.10.weight': [0.000012, '2D']


lr_schedulers:
  # Learning rate decay scheduler
   pruning_lr:
     class: ExponentialLR
     gamma: 0.9

policies:
  - pruner:
      instance_name : 'my_pruner'
    starting_epoch: 0
    ending_epoch: 38
    frequency: 2

  - regularizer:
      instance_name: '2d_groups_regularizer'
    starting_epoch: 0
    ending_epoch: 38
    frequency: 1

  - lr_scheduler:
      instance_name: pruning_lr
    starting_epoch: 24
    ending_epoch: 200
    frequency: 1

```
## Quantization-Aware训练

xxx<div id=#anchor>锚点</div>


与修剪器和正则化器类似，在调度器YAML中指定量化器将跟随Quantizer类的构造函数参数(请参阅此处的详细信息)。注意，每个YAML只能定义一个量化器实例。
让我们看一个例子：
```yaml
quantizers:
  dorefa_quantizer:
    class: DorefaQuantizer
    bits_activations: 8
    bits_weights: 4
    overrides:
      conv1:
        bits_weights: null
        bits_activations: null
      relu1:
        bits_weights: null
        bits_activations: null
      final_relu:
        bits_weights: null
        bits_activations: null
      fc:
        bits_weights: null
        bits_activations: null
```
+ 我们在这里实例化的具体量化方法是DorefaQuantizer。
+ 然后为激活和权重定义默认的位宽(在本例中分别为8位和4位)。
+ 然后，我们定义覆盖映射。在上面的例子中，我们选择不量化模型的第一层和最后一层。对于DorefaQuantizer,权重作为卷积/FC层的一部分进行量化，但激活在单独的层中进行量化,取代原始模型中的ReLU层(记住——尽管我们用自己的量化模块取代了ReLU模块,但模块的名称是不改变的)。因此，总的来说，我们需要引用具有参数conv1的第一个层，第一个激活层relu1，最后一个激活层final_relu
+ 指定null意味着“不要量化”。
+ 注意，对于量化器，我们引用模块的名称，而不是参数的名称，就像我们对修剪器和正则化器所做的那样。


###使用正则表达式定义层组的覆盖
假设我们的模型中有一个名为block1的子模块，它包含多个卷积层，我们想要量化这些层，比如说，2比特。卷积层被称为conv1, conv2，等等。在这种情况下，我们将定义如下:
```yaml
overrides:
  'block1\.conv*':
    bits_weights: 2
    bits_activations: null
```
+ **正则表达式注意**:记住点(```.```)是正则表达式中的元字符(即保留字符)。因此，为了匹配在PyTorch模块名称中分隔子模块的实际点字符，我们需要对其进行转义:\。

重叠模式也是可能的，它允许为一组层定义一些覆盖，也允许为不同的覆盖定义“单独的”特定层。例如，让我们来看最后一个示例并为```block1.conv1```配置一个不同的覆盖
```yaml
overrides:
  'block1\.conv1':
    bits_weights: 4
    bits_activations: null
  'block1\.conv*':
    bits_weights: 2
    bits_activations: null
```
+ 重要提示:模式评估迫切-第一场比赛获胜。因此，要使用“广泛的”模式和如上所示的更“具体的”模式恰当地量化一个模型，请确保在广泛的模式之前列出特定的模式。

在训练过程中控制量化程序的QuantizationPolicy实际上是非常简单的。它所做的只是在初始化量化器时调用其prepare_model()函数，然后第一次调用quantize_params()。然后，在每个epoch结束时，在权重的浮点副本被更新之后，它再次调用quantize_params()函数。

```yaml
policies:
  - quantizer:
      instance_name: dorefa_quantizer
      starting_epoch: 0
      ending_epoch: 200
      frequency: 1
```
+ 重要提示:正如这里提到的，由于量化器修改模型的参数(假设在循环中使用了量化训练)，所以在调用优化器之前必须执行对prepare_model()的调用。因此，当前量化策略的起始epoch必须为0，否则量化过程将不能按预期工作。
如果一个人想做一个“warm-startup”(或“boot-strapping”),在完全精确下训练几个epochs,然后再开始量化,现在唯一的办法就是执行一个单独的运行boot-strapped权重,并执行第二个并恢复boot-strapped的checkpoint。

##训练后量化
训练后量化不同于这里描述的其他技术。由于它不在培训期间执行，因此不需要任何策略或调度程序。目前，用于训练后量化的唯一方法是基于范围的线性量化。使用这种方法量化一个模型，需要添加两行代码:
```
quantizer = distiller.quantization.PostTrainLinearQuantizer(model, <quantizer arguments>)
quantizer.prepare_model()
# Execute evaluation on model as usual
```
有关可用参数的详细信息，请参阅[range_linear.py](https://github.com/NervanaSystems/distiller/blob/master/distiller/quantization/range_linear.py)中的有关```PostTrainLinearQuantizer```的文档。<br>
除了用参数直接实例化量化器外，还可以从YAML文件配置量化器。YAML文件的语法与上面量化感知训练部分中看到的完全相同。毫不奇怪，定义的类必须是```PostTrainLinearQuantizer```，并且忽略YAML文件中定义的任何其他组件或策略。下面我们将看到如何以这种方式创建量化器。

如果需要更多的可配置性，可以使用一个helper函数来添加一组命令行参数来配置量化器:
```
parser = argparse.ArgumentParser()
distiller.quantization.add_post_train_quant_args(parser)
args = parser.parse_args()
```
这些是可用的命令行参数:
```
Arguments controlling quantization at evaluation time ("post-training quantization"):
  --quantize-eval, --qe
                        Apply linear quantization to model before evaluation.
                        Applicable only if --evaluate is also set
  --qe-calibration PORTION_OF_TEST_SET
                        Run the model in evaluation mode on the specified
                        portion of the test dataset and collect statistics.
                        Ignores all other 'qe--*' arguments
  --qe-mode QE_MODE, --qem QE_MODE
                        Linear quantization mode. Choices: sym | asym_s |
                        asym_u
  --qe-bits-acts NUM_BITS, --qeba NUM_BITS
                        Number of bits for quantization of activations
  --qe-bits-wts NUM_BITS, --qebw NUM_BITS
                        Number of bits for quantization of weights
  --qe-bits-accum NUM_BITS
                        Number of bits for quantization of the accumulator
  --qe-clip-acts QE_CLIP_ACTS, --qeca QE_CLIP_ACTS
                        Activations clipping mode. Choices: none | avg | n_std
  --qe-clip-n-stds QE_CLIP_N_STDS
                        When qe-clip-acts is set to 'n_std', this is the
                        number of standard deviations to use
  --qe-no-clip-layers LAYER_NAME [LAYER_NAME ...], --qencl LAYER_NAME [LAYER_NAME ...]
                        List of layer names for which not to clip activations.
                        Applicable only if --qe-clip-acts is not 'none'
  --qe-per-channel, --qepc
                        Enable per-channel quantization of weights (per output
                        channel)
  --qe-scale-approx-bits NUM_BITS, --qesab NUM_BITS
                        Enable scale factor approximation using integer
                        multiply + bit shift, and uset his number of bits to
                        use for the integer multiplier
  --qe-stats-file PATH  Path to YAML file with calibration stats. If not
                        given, dynamic quantization will be run (Note that not
                        all layer types are supported for dynamic
                        quantization)
  --qe-config-file PATH
                        Path to YAML file containing configuration for
                        PostTrainLinearQuantizer (if present, all other --qe*
                        arguments are ignored)
```
(注意```——quantize-eval```和```——qs-calibration```是互斥的。)
当使用这些命令行参数时，量化器可以被调用如下:
```
if args.quantize_eval:
    quantizer = distiller.quantization.PostTrainLinearQuantizer.from_args(model, args)
    quantizer.prepare_model()
    # Execute evaluation on model as usual
```

注意，命令行参数没有公开量化器的overrides参数，该参数允许细粒度地控制每一层的量化方式。要利用这个功能，可以配置一个YAML文件。
要查看使用中这些命令行参数的集成，请参见[图像分类示例](https://github.com/NervanaSystems/distiller/blob/master/examples/classifier_compression/compress_classifier.py)。例如，训练后量化的调用见[这里](https://github.com/NervanaSystems/distiller/blob/master/examples/quantization/post_train_quant)。
###收集用于量化的统计信息
要收集可用于静态量化激活的生成统计信息，请执行以下操作(假设使用了上面所示的命令行参数```——q-calibration```，它指定用于生成统计信息的批数):
```
if args.qe_calibration:
    distiller.utils.assign_layer_fq_names(model)
    msglogger.info("Generating quantization calibration stats based on {0} users".format(args.qe_calibration))
    collector = distiller.data_loggers.QuantCalibrationStatsCollector(model)
    with collector_context(collector):
        # Here call your model evaluation function, making sure to execute only
        # the portion of the dataset specified by the qe_calibration argument
    yaml_path = 'some/dir/quantization_stats.yaml'
    collector.save(yaml_path)
```
然后可以使用'——qs-stats-file参数提供genreated YAML stats文件。在[这里](https://github.com/NervanaSystems/distiller/blob/master/examples/quantization/post_train_quant/stats/resnet18_quant_stats.yaml)可以找到一个生成stats文件的示例。

##修剪精细控制
有时，默认的剪枝过程不能满足我们的需要，我们需要对剪枝过程进行更精细的控制(例如屏蔽、梯度处理和权重更新)。下面我们将解释精细控制配置的数学原理和细微差别。
###设定问题
我们将一个DNN的权值表示为集合
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>&#x03B8;<!-- θ --></mi>
  <mo>=</mo>
  <mrow>
    <mo>{</mo>
    <mrow>
      <msub>
        <mi>&#x03B8;<!-- θ --></mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mi>l</mi>
        </mrow>
      </msub>
      <mo>:</mo>
      <mn>0</mn>
      <mo>&#x2264;<!-- ≤ --></mo>
      <mi>l</mi>
      <mo>&#x2264;<!-- ≤ -->:</mo>
      <mi>L</mi>
    </mrow>
    <mo>}</mo>
  </mrow>
</math>

式中，θl表示l层网络中l层的参数张量(权值和偏差)。通常我们不会因为偏见很小和相对重要而去修剪它们。因此，我们只考虑网络权值(也称网络连接):

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>W</mi>
  <mo>=</mo>
  <mrow>
    <mo>{</mo>
    <mrow>
      <msub>
        <mi>W</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mi>l</mi>
        </mrow>
      </msub>
      <mo>:</mo>
      <mn>0</mn>
      <mo>&#x2264;<!-- ≤ --></mo>
      <mi>l</mi>
      <mo>&#x2264;<!-- ≤ -->:</mo>
      <mi>L</mi>
    </mrow>
    <mo>}</mo>
  </mrow>
</math>

我们希望在某些性能约束(例如精度)下优化某些目标(例如，在推理模式下执行网络所需的能量最小化)，我们通过最大化网络权值的稀疏性(有时在某些选定的稀疏模式约束下)来实现这一点。
我们将剪枝形式化为一个三步动作:
1. 生成一个mask——其中我们定义了每个层的稀疏诱导函数Pl
#TODO
https://nervanasystems.github.io/distiller/schedule.html#pruning-fine-control

##模型蒸馏

知识蒸馏(参见这里)也作为策略实现，应该添加到调度器中。但是，对于当前实现，不能像上面描述的其他策略那样在YAML文件中定义它。
为了使这种方法集成到应用程序中更容易一些，可以使用一个助手函数，它将添加一组与知识蒸馏相关的命令行参数:
```python
import argparse
import distiller

parser = argparse.ArgumentParser()
distiller.knowledge_distillation.add_distillation_args(parser)
```
(add_distillation_args函数接受一些可选参数，详细信息请参阅在distiller/ knowledge_distillib .py上的实现)<br>
下面是这个函数给出的命令行参数:

```shell script
Knowledge Distillation Training Arguments:
  --kd-teacher ARCH     Model architecture for teacher model
  --kd-pretrained       Use pre-trained model for teacher
  --kd-resume PATH      Path to checkpoint from which to load teacher weights
  --kd-temperature TEMP, --kd-temp TEMP
                        Knowledge distillation softmax temperature
  --kd-distill-wt WEIGHT, --kd-dw WEIGHT
                        Weight for distillation loss (student vs. teacher soft
                        targets)
  --kd-student-wt WEIGHT, --kd-sw WEIGHT
                        Weight for student vs. labels loss
  --kd-teacher-wt WEIGHT, --kd-tw WEIGHT
                        Weight for teacher vs. labels loss
  --kd-start-epoch EPOCH_NUM
                        Epoch from which to enable distillation
```
一旦参数被解析，一些初始化代码是必需的，类似如下:

```python
# Assuming:
# "args" variable holds command line arguments
# "model" variable holds the model we're going to train, that is - the student model
# "compression_scheduler" variable holds a CompressionScheduler instance
import args
args.kd_policy = None
if args.kd_teacher:
    # Create teacher model - replace this with your model creation code
    teacher = create_model(args.kd_pretrained, args.dataset, args.kd_teacher, device_ids=args.gpus)
    if args.kd_resume:
        teacher, _, _ = apputils.load_checkpoint(teacher, chkpt_file=args.kd_resume)

    # Create policy and add to scheduler
    dlw = distiller.DistillationLossWeights(args.kd_distill_wt, args.kd_student_wt, args.kd_teacher_wt)
    args.kd_policy = distiller.KnowledgeDistillationPolicy(model, teacher, args.kd_temp, dlw)
    compression_scheduler.add_policy(args.kd_policy, starting_epoch=args.kd_start_epoch, ending_epoch=args.epochs,
                                     frequency=1)
```
最后，在训练循环中，我们还需要通过教师模型进行正向传播。KnowledgeDistillationPolicy类保留对student和teacher模型的引用，并公开一个forward函数，该函数在这两个模型上执行前向传播。由于这不是一个标准的策略回调，我们需要从我们的训练循环手动调用这个函数，如下所示:

```python
if args.kd_policy is None:
    # Revert to a "normal" forward-prop call if no knowledge distillation policy is present
    output = model(input_var)
else:
    output = args.kd_policy.forward(input_var)
```

To see this integration in action, take a look at the image classification sample at examples/classifier_compression/compress_classifier.py.

要查看集成的实际效果，请查看示例/classifier_compression/compress_classifier.py中的图像分类示例。